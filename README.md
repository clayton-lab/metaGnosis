# metaGnosis: Clayton Lab Metagenomics Processing Pipeline

## Overview
Snakemake pipeline for processing of metagenomic data from the lab. It accepts raw fastq files of metagenomic data, quality filters them, and removes reads that map to the host genome. From there, it allows reads to be processed in an assembly-based or assembly-free manner. In either case, the final outputs of the pipeline are quantified abundance of bacterial taxa, genes, and functional pathways. Future development goals include improved handling of non-bacterial reads (e.g. fungi, protists), greater flexibility and user-friendliness, and the addition of modules from pre-built genomics workflows to further process host-mapped reads.

## Quick Start Guide

### Install

First, clone this github repository:

```
$ git clone git@github.com:clayton-lab/metaGnosis.git
cd metaGnosis
```

We recommend installing and using mamba (**Not Necessary for Users on HPC Environments like HCC**):

```
$ conda install -c conda-forge mamba
```

**For users on HPC environments where mamba is already installed (i.e., HCC)**, use the module load command instead:

```
$ module load mamba/1.5     # The /1.5 suffix is only necessary if a specific version is required).
```

Then install the snakemake version for this workflow using mamba:

```
$ mamba env create -n snakemake-env -f resources/env/snakemake.yaml
$ conda activate snakemake
```



### Update Files

Now you can update three files that are located in the `./resources/config` directory.

The first two files are `samples.txt` and `units.txt`. The `units.txt`file is a manifest file that should have only 4 columns, and each row should correspond to a sample found in the `samples.txt` file. The first column, "Sample_ID", should be all or a subset of the "Sample_ID" column in the `samples.txt` file. The second column, "Unit_ID", should denote which analysis block each sample belongs to. In our case, we use sequencing run/lane, but you may use other information based on your experimental design. The third and fourth columns (named "R1" and "R2", respectively) should include the full file paths to the forward and reverse fastq files for that sample. Only pair-end reads are compatible with the pipeline in its current iteration, but making the pipeline compatible with single-end reads is a goal for future versions.

The `samples.txt` file is your basic metadata file, with each row representing a sample in your dataset and each column containing the corresponding information about that sample. Samples.txt uses 3 columns — Sample_ID, Contig_ID, and Mapping_Group — to infer which assembly and binning strategies to use for each sample. The first column should be named "Sample_ID" and should contain the name for each sample. The name specified in Sample_ID will be used to name all subsequent read data (i.e., processed sequence files) generated by the pipeline. The next column should be named "Contig_ID" and — in addition to being used to name all subsequent assembly and binning data generated by the pipeline — is also used to specify which assembly strategy to use for each sample (co-assembly, individual assembly, or no assembly). The pipeline infers that any Sample_IDs with the same Contig_ID (e.g., Sample_ID=s1,s2; Contig_ID=s,s) are used to create a co-assembly. Individual assembly is instead used if a Sample_ID has a unique Contig_ID (e.g., Sample_ID=s1,s2; Contig_ID=s1,s2). No assembly will be used for a Sample_ID whose corresponding Contig_ID is empty (specified by "" or None). The third column should be named "Mapping_Group" and is used to map sample reads (inferred from Sample_ID) to sample contigs (inferred from Contig_ID). Bin names correspond to the contigs (Contig_IDs) — rather than the sample reads (Sample_IDs) — used to generate them, meaning that the values specified by Mapping_Group are used purely for grouping purposes. Similar to assembly strategies, this adds flexibility by allowing the user to choose from multiple binning strategies. Sample reads (Sample_IDs) are mapped to all sample assemblies (Contig_IDs) in the same Mapping_Group for binning, even if no assemblies for a given sample were created.

An example will help in understanding the possibilities afforded by using samples.txt to specify defired assembly and binning behavior. Imagine a scenario where there are 4 samples with respective Sample_IDs=s1,s2,s3,s4, Contig_IDs=s,s,s3,None, and Mapping_Group=g1,g1,g2,g2. After processing reads from each sample (named according to Sample_ID column), the pipeline will infer assembly behavior based on the Contig_ID column. In this scenario, reads from samples s1 and s2 will be used to create a co-assembly called s, reads from sample s3 will be used to create an individual (s3-specific) assembly called s3, and no assembly will be created from sample s4's reads. After read assembly, the pipeline will infer binning behavior based on the Mapping_Group column. In this scenario, reads from samples s1 and s2 will be mapped to assembly s, and reads from samples s3 and s4 will be mapped to assembly s3 for binning. If sample s4 were instead in its own unique Mapping_Group=s4, only s3 reads would be mapped to assembly s3, and sample s4 would remain untouched after read processing. In this toy scenario, 3 assembly strategies (individual assembly, co-assembly, and no assembly) and 4 binning strategies (1-to-1 mapping, many-to-1 mapping, many-to-prototype mapping, no mapping) can be performed simultaneously in the same run. It is worth noting that, unlike in units.txt, the current version of the pipeline does not allow duplicate Sample_IDs to be used. However, a future goal of this pipeline is to bypass that restriction, allowing the same sample to be a member of multiple Mapping_Groups. This would allow simultaneous individual and co-assemblies, and mappings, to be performed from the same sample.

The last file to update is the the `config.yaml` file. This is where you can select the parameters for each step in the analysis pipeline. Parameters can be specified by adding/deleting them from the config.yaml file, or by simply commenting undesired lines out by adding a "#" prefix. Refer to the documentation for each tool individually for more information. Also, be sure to change the NCBI GenBank Accession number to your host of interest. This accession number will be fetched by the pipeline automatically, removing the hassle of locating and downloading the correct genome fasta file.

NOTE: You can select which metagenomic assembler you want to use under the the "assemblers:" header. The current options are [metaSPAdes](https://cab.spbu.ru/software/meta-spades/) and [MEGAHIT](https://github.com/voutcn/megahit) Simply delete the assembler you don't want to use. Otherwise both will run.

### Run the Pipeline

After you have updated the files described above, you can start the pipeline. First run:
```
conda install -n base -c conda-forge mamba
```

Then begin the run using:
```
snakemake --cores 8 --use-conda
```
The first time you run this, it may take longer to set up your conda environment. Be sure to select the appropriate number of cores for your analysis.
